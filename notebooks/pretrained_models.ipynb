{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt =\"\"\"The global recovery is slowing amid widening divergences among economic sectors and regions\n",
    "\n",
    "Global growth is projected to fall from an estimated 3.5 percent in 2022 to 3.0 percent in both 2023 and 2024. While the forecast for 2023 is modestly higher than predicted in the April 2023 World Economic Outlook (WEO), it remains weak by historical standards. The rise in central bank policy rates to fight inflation continues to weigh on economic activity. Global headline inflation is expected to fall from 8.7 percent in 2022 to 6.8 percent in 2023 and 5.2 percent in 2024. Underlying (core) inflation is projected to decline more gradually, and forecasts for inflation in 2024 have been revised upward.\n",
    "\n",
    "The recent resolution of the US debt ceiling standoff and, earlier this year, strong action by authorities to contain turbulence in US and Swiss banking reduced the immediate risks of financial sector turmoil. This moderated adverse risks to the outlook. However, the balance of risks to global growth remains tilted to the downside. Inflation could remain high and even rise if further shocks occur, including those from an intensification of the war in Ukraine and extreme weather-related events, triggering more restrictive monetary policy. Financial sector turbulence could resume as markets adjust to further policy tightening by central banks. Chinaâ€™s recovery could slow, in part as a result of unresolved real estate problems, with negative cross-border spillovers. Sovereign debt distress could spread to a wider group of economies. On the upside, inflation could fall faster than expected, reducing the need for tight monetary policy, and domestic demand could again prove more resilient.\n",
    "\n",
    "In most economies, the priority remains achieving sustained disinflation while ensuring financial stability. Therefore, central banks should remain focused on restoring price stability and strengthen financial supervision and risk monitoring. Should market strains materialize, countries should provide liquidity promptly while mitigating the possibility of moral hazard. They should also build fiscal buffers, with the composition of fiscal adjustment ensuring targeted support for the most vulnerable. Improvements to the supply side of the economy would facilitate fiscal consolidation and a smoother decline\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer(txt, return_tensors=\"pt\", \n",
    "                max_length=512,\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                add_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  1996,  3795,  7233,  2003, 18068, 13463, 17973, 17856, 17905,\n",
       "          2015,  2426,  3171, 11105,  1998,  4655,  3795,  3930,  2003, 11310,\n",
       "          2000,  2991,  2013,  2019,  4358,  1017,  1012,  1019,  3867,  1999,\n",
       "         16798,  2475,  2000,  1017,  1012,  1014,  3867,  1999,  2119, 16798,\n",
       "          2509,  1998, 16798,  2549,  1012,  2096,  1996, 19939,  2005, 16798,\n",
       "          2509,  2003, 10754,  2135,  3020,  2084, 10173,  1999,  1996,  2258,\n",
       "         16798,  2509,  2088,  3171, 17680,  1006,  2057,  2080,  1007,  1010,\n",
       "          2009,  3464,  5410,  2011,  3439,  4781,  1012,  1996,  4125,  1999,\n",
       "          2430,  2924,  3343,  6165,  2000,  2954, 14200,  4247,  2000, 17042,\n",
       "          2006,  3171,  4023,  1012,  3795, 17653, 14200,  2003,  3517,  2000,\n",
       "          2991,  2013,  1022,  1012,  1021,  3867,  1999, 16798,  2475,  2000,\n",
       "          1020,  1012,  1022,  3867,  1999, 16798,  2509,  1998,  1019,  1012,\n",
       "          1016,  3867,  1999, 16798,  2549,  1012, 10318,  1006,  4563,  1007,\n",
       "         14200,  2003, 11310,  2000,  6689,  2062,  6360,  1010,  1998, 19939,\n",
       "          2015,  2005, 14200,  1999, 16798,  2549,  2031,  2042,  8001, 10745,\n",
       "          1012,  1996,  3522,  5813,  1997,  1996,  2149,  7016,  5894,  3233,\n",
       "          7245,  1998,  1010,  3041,  2023,  2095,  1010,  2844,  2895,  2011,\n",
       "          4614,  2000,  5383, 29083,  1999,  2149,  1998,  5364,  8169,  4359,\n",
       "          1996,  6234, 10831,  1997,  3361,  4753, 17930,  1012,  2023,  8777,\n",
       "          2094, 15316, 10831,  2000,  1996, 17680,  1012,  2174,  1010,  1996,\n",
       "          5703,  1997, 10831,  2000,  3795,  3930,  3464,  9939,  2000,  1996,\n",
       "         12482,  5178,  1012, 14200,  2071,  3961,  2152,  1998,  2130,  4125,\n",
       "          2065,  2582, 28215,  5258,  1010,  2164,  2216,  2013,  2019, 20014,\n",
       "          6132,  9031,  1997,  1996,  2162,  1999,  5924,  1998,  6034,  4633,\n",
       "          1011,  3141,  2824,  1010, 29170,  2062, 25986, 12194,  3343,  1012,\n",
       "          3361,  4753, 29083,  2071, 13746,  2004,  6089, 14171,  2000,  2582,\n",
       "          3343, 18711,  2011,  2430,  5085,  1012,  2859,  1521,  1055,  7233,\n",
       "          2071,  4030,  1010,  1999,  2112,  2004,  1037,  2765,  1997,  4895,\n",
       "          6072, 16116,  2613,  3776,  3471,  1010,  2007,  4997,  2892,  1011,\n",
       "          3675, 14437, 24302,  1012, 11074,  7016, 12893,  2071,  3659,  2000,\n",
       "          1037,  7289,  2177,  1997, 18730,  1012,  2006,  1996, 14961,  1010,\n",
       "         14200,  2071,  2991,  5514,  2084,  3517,  1010,  8161,  1996,  2342,\n",
       "          2005,  4389, 12194,  3343,  1010,  1998,  4968,  5157,  2071,  2153,\n",
       "          6011,  2062, 24501, 18622,  4765,  1012,  1999,  2087, 18730,  1010,\n",
       "          1996,  9470,  3464, 10910,  8760,  4487, 11493, 10258,  3370,  2096,\n",
       "         12725,  3361,  9211,  1012,  3568,  1010,  2430,  5085,  2323,  3961,\n",
       "          4208,  2006, 16487,  3976,  9211,  1998, 12919,  3361, 10429,  1998,\n",
       "          3891,  8822,  1012,  2323,  3006, 18859,  3430,  4697,  1010,  3032,\n",
       "          2323,  3073,  6381,  3012, 13364,  2096, 10210, 13340,  3436,  1996,\n",
       "          6061,  1997,  7191, 15559,  1012,  2027,  2323,  2036,  3857, 10807,\n",
       "         17698,  2015,  1010,  2007,  1996,  5512,  1997, 10807, 19037, 12725,\n",
       "          9416,  2490,  2005,  1996,  2087,  8211,  1012,  8377,  2000,  1996,\n",
       "          4425,  2217,  1997,  1996,  4610,  2052, 10956, 10807, 17439,  1998,\n",
       "          1037,  5744,  2121,  6689,   102,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[CLS] = 101\n",
    "[SEP] = 102\n",
    "[MASK]= 103\n",
    "[UNK] = 100\n",
    "[PAD] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(**tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[ 3.0746, -2.5698]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.0746, -2.5698]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = F.softmax(outputs[0], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9965, 0.0035]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "preds = torch.argmax(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt2 = \"\"\" i feel good\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens2 = tokenizer(txt2, return_tensors=\"pt\",\n",
    "                    max_length=512,\n",
    "                    truncation=True,\n",
    "                    padding=\"max_length\",\n",
    "                    add_special_tokens=True)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "output2 = model(**tokens2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-4.2158,  4.5907]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs2 = F.softmax(output2[0], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.4973e-04, 9.9985e-01]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds2 = torch.argmax(probs2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds2.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
